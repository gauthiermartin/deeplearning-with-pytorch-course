{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP0zEPQwxfqtvKm9OKWwNre",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauthiermartin/pytorch-deep-learning-course/blob/main/10_Introduction_to_pytorch_2_0_and_torch_compile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch 2.0 Quick Intro\n",
        "\n",
        "* Reference Book Chapter - https://www.learnpytorch.io/pytorch_2_intro/\n",
        "* PyTorch 2.0 Release Notes - https://pytorch.org/blog/pytorch-2.0-release/"
      ],
      "metadata": {
        "id": "FE0y9iJWytMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jayJUfLm0lXa",
        "outputId": "4ce31591-38be-4e32-94db-7918bc04ee96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g69rYzSA0rQA",
        "outputId": "e1f142ac-efc2-455a-e69d-0d85ee47b5ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  2 11:39:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick code examples"
      ],
      "metadata": {
        "id": "xeYmta2L2Ylg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before PyTorch 2.0"
      ],
      "metadata": {
        "id": "ZKzO_TYg2fsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50()"
      ],
      "metadata": {
        "id": "lZnCVaf_2jhU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Pytorch 2.0"
      ],
      "metadata": {
        "id": "ZFzPzOIP2rt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50() # note: this could be any model\n",
        "compiled_model = torch.compile(model)"
      ],
      "metadata": {
        "id": "3I9I4CMm2urR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting Setup"
      ],
      "metadata": {
        "id": "TSdv6DmE3AOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Check PyTorch version\n",
        "pt_version = torch.__version__\n",
        "print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "\n",
        "# Install PyTorch 2.0 if necessary\n",
        "if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n",
        "          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n",
        "    import torch\n",
        "    pt_version = torch.__version__\n",
        "    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "else:\n",
        "    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGaF3PeI3SyN",
        "outputId": "5bf99b40-202b-467a-e689-d650771d791b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Current PyTorch version: 2.1.0+cu118 (should be 2.x+)\n",
            "[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get GPU info\n",
        "\n",
        "Why get GPU info ?  \n",
        "\n",
        "Becasue PyTorch 2.0 feature leverage the features on the new NVIDIA GPU.\n",
        "\n",
        "Well what's a newer NVIDIA GPU ?\n",
        "\n",
        "To find out NVIDIA GPU Compatibility Score - https://developer.nvidia.com/cuda-gpus\n",
        "\n",
        "\n",
        "If your GPU has a score of 8.0+, it can leverage **most** of the new PyTorch 2.0 Feature, but some of these feature will still have an impact on lower score GPU but impact will be less.\n",
        "\n",
        "**Note:** If you are wondering witch gpu to use for deep learning - Checkout Tim Dettmers blog post \"Which GPU for deep learning\" - https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/"
      ],
      "metadata": {
        "id": "HpKKWrWF3bZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6PgbC0k-ZIM",
        "outputId": "70b6a148-88a2-4c9b-cf4e-d11c4a3e0f10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU name: NVIDIA_A100-SXM4-40GB\n",
            "GPU capability score: (8, 0)\n",
            "GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\n",
            "GPU information:\n",
            "Fri Nov  3 11:40:59 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    42W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Globaly set devices\n",
        "\n",
        "Previously, we've set the device of our tensors/models using `.to(device)`\n",
        "\n",
        "* `tensor.to(device)`\n",
        "* `model.to(device)`\n",
        "\n",
        "But in PyTorch 2.0 it is possible to set the device with a context manager as well as a global device\n",
        "\n",
        "Docs:\n",
        "- https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html?highlight=device%20context%20manager\n",
        "- https://pytorch.org/docs/stable/generated/torch.set_default_device.html"
      ],
      "metadata": {
        "id": "ojXO3iST_V5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "\n",
        "# Set the device with context manager (Requires PyTorch 2.x +)\n",
        "with torch.device(device):\n",
        "    # All tensors / models created within context manager will be on device without using the `to` method\n",
        "    layer = torch.nn.Linear(10, 10)\n",
        "\n",
        "    print(f\"Model on Device: {layer.weight.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHJO9c1DBDXc",
        "outputId": "43a61648-fdfe-4626-b058-ba936cfe0892"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device globally (Requires PyTorch 2.x +)\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# All tensors / models created within context manager will be on device without using the `to` method\n",
        "layer = torch.nn.Linear(10, 10)\n",
        "\n",
        "print(f\"Model on Device: {layer.weight.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW6uAbnxCJum",
        "outputId": "cb5610e9-f37e-45c7-ca9a-c949f474b43e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device globally (Requires PyTorch 2.x +)\n",
        "torch.set_default_device(\"cpu\")\n",
        "\n",
        "# All tensors / models created within context manager will be on device without using the `to` method\n",
        "layer = torch.nn.Linear(10, 10)\n",
        "\n",
        "print(f\"Model on Device: {layer.weight.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cAobQ4sCcOQ",
        "outputId": "159faaef-a115-4323-d0dc-5da6e949f517"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting up the experiments\n",
        "\n",
        "Time to test speed!\n",
        "\n",
        "To keep simple we will run 4 examples\n",
        "\n",
        "\n",
        "* Model: ResNet 50 from torchvision\n",
        "* Data: CIFA10 from torchvision\n",
        "* Epochs: 4 single run and 3x5 (multi-run)\n",
        "* Batch Size : 128\n",
        "\n",
        "Note you may want to change this depending on the available GPU memory available\n",
        "\n"
      ],
      "metadata": {
        "id": "-VbQOosyCf9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WhG1JH4Ctl5",
        "outputId": "19bc19c8-cb06-4ac8-9ca3-d2e24d99ded4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov  3 11:59:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    47W / 400W |   1137MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set the target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.set_default_device(device)\n",
        "print(f\"Using Default Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_D4rMSjDeyz",
        "outputId": "da5a0258-e406-4933-955b-ee35a8c0200b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.0+cu118\n",
            "TorchVision version: 0.16.0+cu118\n",
            "Using Default Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model weights and transforms\n",
        "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n",
        "transforms = model_weights.transforms()\n",
        "\n",
        "# Setup model\n",
        "model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "# Count the number of parameters in the model\n",
        "total_params = sum(\n",
        "    param.numel() for param in model.parameters() # <- all params\n",
        ")\n",
        "\n",
        "total_trainable_parms = sum(\n",
        "    param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",
        ")\n",
        "\n",
        "print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n",
        "print(f\"Total trainable parameters of model: {total_trainable_parms}\")\n",
        "print(f\"Model transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYH7zIbTDi-2",
        "outputId": "bc314c38-bfc0-4cf0-da67-46d39893fe05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\n",
            "Total trainable parameters of model: 25557032\n",
            "Model transforms:\n",
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[232]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Pytorch 2.x *relative* speed up will most noticable when as much of the GPU is possible us being used. This means a larger model (more trainable parameters) will have a larger relative speedup.\n"
      ],
      "metadata": {
        "id": "msswcQE0Ejwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "\n",
        "  model_weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
        "  transforms = model_weights.transforms()\n",
        "\n",
        "  # Setup model\n",
        "  model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "  model.fc = torch.nn.Linear(in_features=2048, out_features=10)\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "B7VfH7JBFuir"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, transforms = create_model()\n",
        "transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgW0cpafGNz6",
        "outputId": "13256ac4-3350-4e53-8c17-8f32caa41975"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[232]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Speedups are most noticable when a large portion of the GPU is being used\n",
        "\n",
        "Since modern GPUs are so *fast* at performing operations, you will often notice the majority of *relative* speedups when as much data as possible is on the GPU.\n",
        "\n",
        "In practice, you generally want to use as much of the GPU memory as possible.\n",
        "\n",
        "* Increasing batch size - With a larger memory GPU you can increase batch size to 64, 128, 256, 512 ...\n",
        "* Increasing data size - Use 224x224 image vs 32x32 or event 336x336\n",
        "* Increase the model size - from 1M params to 10M params\n",
        "* Decrease data transfer - Since bandwidth costs (transfering data) will slow down a GPU (because it wants to compute on data)\n",
        "\n",
        "As a results of doing the above, your relative speedup should be better\n",
        "\n",
        "E.g. Overall training time may take longer but not lineraly.\n",
        "\n",
        "\n",
        "Resource\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gwP0H25mGYhH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JR_MHYxgHxRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}